{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote the optimal state by $s_*$, then $v(s_*) = E \\left[ G_t \\mid S_t = s \\right] = E \\left[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} \\cdots \\mid S_t = s \\right]$\n",
    "\n",
    "Note that we have the optimal policy in figure 3.5, so we can determine $R_t$ for any futures $t$. The optimal policy says if we are at the optimal state, we will move to $A'$ to get a reward of +10, after that we have to move 4 steps back to $A$. So $v(s_*) = E [ G_t \\mid s] =  = 10 + 0 \\gamma + 0 \\gamma^2 + 0 \\gamma^3 + 0 \\gamma^4 + 10 \\gamma^5 + \\cdots = \\frac{10}{1 - \\gamma^5} \\approx 24.419 $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
