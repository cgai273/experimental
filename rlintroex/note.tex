\documentclass[twocolumn, 10pt]{article}

\usepackage[top=0.5in, bottom=1in, left=0.12in, right=0.2in]{geometry}
\usepackage{amsmath}
\usepackage{tcolorbox}
\setlength{\columnsep}{1cm}
\setlength{\columnseprule}{0.2pt}
 

\newtcolorbox{codeblock}{colback=white, colframe=white!20!black}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\actionset}{ \mathcal{A}}
\newcommand{\stateset}{\mathcal{S}}
\newcommand{\termstateset}{\mathcal{S}^+}
\newcommand{\rewardset}{\mathcal{R}}
\newcommand{\Mid}{ \bigg|}

\title{Note for Reinforcement Learning 2nd Edition}

\begin{document}
\maketitle

\section*{Finite Markov Decision Process}

MDP framework consists of an environment  and agent. For $t=0,1,2...t$, agent  receives observed state $S_t  \in  \stateset$ based on which the agent perform an action $A_t \in \actionset$.  The dynamic of the environemtn  then returns a reward $R_{t+1} \in \rewardset$.  This forms a trajectory $S_0,A_0,R_1, S_1,A_1, R_2, \ldots$.  The dynamic for MDP is defined to be 
$$ p(s', r  \mid s, a) = Pr\{S_t = s', R_t = r  \mid  S_{t- 1} = s, A_{t-1} = a \} $$

Several commonly use quantities:

state-transition probability
$$ p(s' \mid  s, a) = \sum_r p(s', r \mid s, a)$$

expected reward for state-action pair
$$
\begin{aligned}
 r(s, a)  &= E[R_t \mid S_{t-1} = s, A_{t - 1} = a]  \\
               &=\sum_{r \in \rewardset} r  \sum_{s' \in \stateset} p(s', r \mid s, a) \\
 \end{aligned}
$$

expected reward for state-action-next-state triples 

$$
\begin{aligned}
r(s, a, s') &= E[R_t \mid S_{t-1} = s, A_{t-1} = a, S_t = s']  \\
                    &= \sum_{r \in \rewardset} r  p(r \mid s, a, s') \\
                    &= \sum_{r \in \rewardset} r  \frac{p(s', r \mid s, a) } {p(s' \mid s, a, s')}
\end{aligned}
$$

For episodic tasks, we have nonterminal states $\stateset$ and terminal states $\termstateset$.  Time of termination $T$ is  a random variable between episodes.  

Returns at time step $t$ is 
$$
\begin{aligned}
G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \\
        &= R_{t+1} + \gamma G_{t+1} \\
\end{aligned}
$$

All reinforcement learning algorithm involve estimating 3 quantities: value function, state-action function.

A policy is a probability distribution on $\actionset$ given $s$ denoted as  $\pi (a \mid s)$.

A value function of a state $s$ under a policy $\pi$ is the expected return when starting in  $s$ and following $\pi$ thereafter.

$$ 
\begin{aligned}
v_{\pi}(s) &= E_{\pi} [G_t \mid S_t = s]  \\
                 &=  E_{\pi} \left[ \sum^{\infty}_{i=0}  \gamma^i R_{t + i + 1} \Mid S_t = s \right],  \hspace{2ex} \mbox{ (for all } s \in \stateset)  \\
                 &=  E_{\pi} \left[ R_{t+1} + \gamma \sum^{\infty}_{i=0} \gamma^i R_{(t+1) + i + 1} \Mid S_t = s \right] \\
                 &=  E_{\pi} [ R_{t+1} \mid S_t = s]  + \gamma  E_{\pi} [ G_{t+1} \mid S_t = s ] \\
                 &=  \sum_{a, r, s'} r p(s',  r \mid s , a) \pi(a \mid s) , \hspace{2ex} \mbox{(Law of Total Expectation)} \\ 
                 &\hspace{3ex} +  \gamma  \sum_{a, s'} E_{\pi}[G_{t + 1} \mid S_{t+1} = s'] p(s' \mid s, a)  \pi (a \mid s)  \\
                 &=  \sum_{a, r, s'} r p( r \mid s , a) \pi(a \mid s)  +  \gamma  \sum_{a, r, s'} v_{\pi}(s')  p(s'
                 , r \mid s, a)  \pi (a \mid s)  \\
                 &= \sum_{a} \pi(a \mid s)  \sum_{r, s'}   p(s', r \mid s, a)   \left[ r  + \gamma  v_{\pi}(s')  \right]
\end{aligned}
$$

The state-action function is the expected return  of taking an action $a$ at state $s$ before following the policy thereafter.
$$
\begin{aligned}
 q_{\pi}(s, a) &= E_{\pi} [G_t \mid S_t = s, A_t = a]  \\
             &= E_{\pi} \left[ \sum^{\infty}_{i=0}  \gamma^i R_{t + i + 1}  \Mid S_t = s , A_t = a\right]  \\
             &= E_{\pi} [R_{t+1} \mid S_t = s, A_t =  a] + \gamma E_{\pi} [ G_{t+1} \mid S_t = s, A_t =  a] \\
             &=  \sum_{s', r} r p(s', r  \mid s, a) + \gamma \sum_{s', a'} E_{\pi} [G_{t+1} \mid s', a'] p(s', a' \mid s, a) \\
             &=  \sum_{s', r} r p(s', r  \mid s, a) + \gamma \sum_{s', a'} q(s', a') p( a' \mid s',  s, a) p(s' \mid  s, a) \\
             &=  \sum_{s', r} r p(s', r  \mid s, a) + \gamma \sum_{s', a', r} q(s', a') \pi(a' \mid s')  p(s', r \mid  s, a) \\
             &=  \sum_{s', r}  p(s', r  \mid s, a) \left [ r + \gamma \sum_{a'} q(s', a') \pi(a' \mid s')  \right]   \hspace{2ex} (*) 
 \end{aligned}
$$

More identities:
$$
\begin{aligned}
 q_{\pi}(s, a) &= E_{\pi} [G_t \mid S_t = s, A_t = a]  \\
 					   &= E_{\pi} [R_{t+1} + \gamma G_{t+1} \mid  s,  a] \\
                       &=   E_{\pi} [R_{t+1} \mid s, a] + \gamma \sum_{s'} E_{\pi}  [G_{t+1} \mid s'] p(s' \mid s, a) \\
                       &=  \sum_{s', r}  r p(s', r  \mid s, a) + \gamma \sum_{s' } v_{\pi}(s')   \sum_{r} p(s', r \mid s, a) \\
                       &=  \sum_{s', r} p(s', r  \mid s, a)  ( r  + \gamma  v_{\pi}(s') ) \\
                       &=   E_{\pi} [R_{t+1}  + \gamma  v_{\pi}(S_{t+1}) \mid s, a] \\
\end{aligned}
$$

\subsubsection*{Optimal policy and value fucntions}

 A policty $\pi' \geq \pi$ iff  $v'_{\pi}(s) \geq v_{\pi}(s), \forall s \in \stateset$. Optimal state value function is defined as $$v_*(s) = \max_{\pi} v_{\pi}(s),  \forall s \in \stateset$$
 
 Similarly,  optimal state action value function is defined as $$ q_*(s, a) = \max_{\pi} q_{\pi}(s, a) $$
   
 Bellman optimality equation for $v_*$  
 $$
\begin{aligned}
v_*(s) & = \max_{a} q_{\pi_*}(s, a) =\max_a E_{\pi} [G_t \mid S_t = s, A_t = a] \\
            & = \max_{a} E_{\pi_*} [R_{t+ 1} + \gamma G_{t+1} \mid S_t = s, A_t = a] \\
            & = \max_{a} [  E_{\pi_*} [R_{t+ 1}  \mid s, a ]  \\
            & + \gamma   \sum_{s', r}  E_{\pi_*} [G_{t+1}  \mid S_{t+1} = s']p(s', r \mid  s, a)]  \\
            & = \max_{a} \left[  \sum_{s', r} r p(s', r \mid s, a)   + \gamma   \sum_{s', r}  v_*(s')p(s', r \mid  s, a) \right]  \\
            & = \max_{a} \sum_{s', r} p(s', r \mid s, a) [r + \gamma v_*(s')] \\
\end{aligned}
$$

Bellman optimality equation for $q_*$
 $q_*(s,a)$ is the value of taking action $a$ at state $s$, then follow optimal policy after. Hence we can also write.
$$ 
\begin{aligned}
q_*(s, a) &= \max_{\pi} q_{\pi} (s, a) \\
&= \max_{\pi} E_{\pi} [R_{t+1} + \gamma G_{t+1} \mid s, a] \\
& = \max_{\pi} \left( E_{\pi}[R_{t+1} \mid s, a]  + \gamma E_{\pi}[ G_{t+1}  \mid s, a]  \right)\\
&= \max_{\pi}  \left( E_{\pi}[R_{t+1} \mid s, a]  +  \gamma \sum_{s'} E_{\pi}[ G_{t+1} \mid s'] p(s' \mid s, a)  \right)\\
&=\left( E_{\pi}[R_{t+1} \mid s, a]  +  \gamma  \max_{\pi}   \sum_{s'} v_{\pi}(s') p(s' \mid s, a)  \right)\\
&= \sum_{s', r }  p(s', r \mid s, a) \left[ r +  \gamma  \max_{\pi}  v_{\pi}(s') p(s', r \mid s, a) \right] \\
&= \sum_{s', r }  p(s', r \mid s, a) \left[ r +  \gamma  v_{*}(s') p(s', r \mid s, a) \right] \\
&= \sum_{s', r }  p(s', r \mid s, a) \left[ r +  \gamma  \max_{a'} q_{*}(s', a') p(s', r \mid s, a) \right] \\
\end{aligned}$$

Once we have $v_*(s)$, optimal policy is just a one step search, it selects the action that maximize the sum of immediate reward and the value of the next state i.e.  $\max\{R_{t+1} + \gamma v_*(S_{t+1})\}$.  If we have $q_*(s, a)$ instead, then for any $s$,  the optimal policy will be just the action that maximize $q_*(s, a)$.

Solving bellman equation is not pratical since we all satisfy the following 3 conditions:.
\begin{enumerate}
  \item Know dynamics of the environemtn (Poker provides partially observed state only)
 \item  Have  enough computation resource (Go has huge state space exceeed the number of atoms in the universe)
 \item  Problem needs to have markov property (Weather predition cannot be based on yesterday only)
\end{enumerate}
 Therefore we typically find approximations to the value functions.
 
 \section*{Solving Bellman Equation with DP}
 
 \subsubsection*{Policy Evaluation}
 Recall that for any policy $\pi$,  $$v_{\pi} (s)  = \sum_{a} \pi(a \mid s)  \sum_{r, s'}   p(s', r \mid s, a)   \left[ r  + \gamma v_{\pi}(s')  \right]$$
 If the environment is known, we can solve it as a set of linear equations. If not, we can approximate $v_{\pi}$ given $\pi$ by initializing $v_0$ and following the iteration: (Note that unique existence and convergence is guaranteed as long as $\gamma < 1$ (e.g.  $||v_{k+1} - v_k||_\infty \rightarrow 0$). )
 $$ \begin{aligned}
  v_{k+1} (s)  &= E_{\pi} [R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t]  \\
                       &=  \sum_{a} \pi(a \mid s)  \sum_{r, s'}   p(s', r \mid s, a)   \left[ r  + \gamma v_{k}(s')  \right] \\
 \end{aligned}$$
  
  Since we are averaging the next states, this is called expected update. There are two ways to update algorithm, "in-place update" where we do a sweep through a single array ($v(s')$ can be the updated value for some $s'$) and "double buffers update" where we keep two arrays, update the back buffer array using the front buffer array and swap the reference. Both will converge. We usually prefer the former.
  
 \subsubsection*{Policy Improvement}
The policy improvement theorem says that if $\pi,  \pi'$ are two deterministic policies such that $$q_{\pi} (s, \pi'(s))  \geq v_{\pi}(s)$$ for all $s \in \stateset$. Then $\pi'$ is better or as good as $\pi$, e.g $$ v_{\pi'}(s) \geq v_{\pi}(s), \hspace{2ex} \forall s \in \stateset$$ (TODO: Derive this).

Given a policy $\pi$, we can compute $v_{\pi}$. And from $v_{\pi}$,  we can find a better policy $\pi'$ with a greedy one step lookahead search. 
$$  
\begin{aligned}
\pi'(s)  &= \argmax_{a} q_{\pi}(s, a) \\
			&= \argmax_{a} E_{\pi|} [ R_{t+ 1} + \gamma v_{\pi} (S_{t+1}) \mid  s, a] \\
			&= \argmax_{a} p(s', r \mid s , a) [r + \gamma v_{\pi}(s')]
\end{aligned}
$$

\subsubsection*{Policy Iteration}
We start with a policy $\pi_0$ and alternate between policy evaluation (E) and policy improvement (I) until it converges:
$$ \pi_0 \overset{E}{\rightarrow} v_0 \overset{I}{\rightarrow} \pi_1 \overset{E}{\rightarrow} \cdots \overset{E}{\rightarrow} v_* \overset{I}{\rightarrow} \pi_* $$
 
 \subsubsection*{Value Iteration}
 Policy evaluation and improvement can  be combined such that one update includes one sweep of policy evaluation and one sweep of policy improvement. We call this value iteration which can be obtained from Bellman's optimality equation directly:
 $$
 \begin{aligned}
 v_{k+1}(s) &= \max_{a} E[R_{t+1} + \gamma v_{k}(S_{t+1}) \mid s, a] \\
                     &= \max_{a} \sum_{s' , r} p(s', r \mid s, a) [r + \gamma v_{k}(s')]
 \end{aligned}
 $$
 We can terminate when the distance between $v_k$ and $v_{k+1}$ is small enough.
\end{document}