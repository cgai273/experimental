\documentclass[twocolumn, 10pt]{article}

\usepackage[top=0.5in, bottom=1in, left=0.5in, right=0.5in]{geometry}
\usepackage{amsmath}
\hyphenpenalty=10000
\setlength{\columnsep}{1cm}
\setlength{\columnseprule}{0.2pt}

\newcommand{\actionset}{ \mathcal{A}}
\newcommand{\stateset}{\mathcal{S}}
\newcommand{\termstateset}{\mathcal{S}^+}
\newcommand{\rewardset}{\mathcal{R}}
\newcommand{\Mid}{ \bigg|}

\title{Note for Reinforcement Learning 2nd Edition}

\begin{document}
\maketitle

\section*{Finite Markov Decision Process}

MDP framework consists of an environment  and agent. For $t=0,1,2...t$, agent  receives observed state $S_t  \in  \stateset$ based on which the agent perform an action $A_t \in \actionset$.  The dynamic of the environemtn  then returns a reward $R_{t+1} \in \rewardset$.  This forms a trajectory $S_0,A_0,R_1, S_1,A_1, R_2, \ldots$.  The dynamic for MDP is defined to be 
$$ p(s', r  \mid s, a) = Pr\{S_t = s', R_t = r  \mid  S_{t- 1} = s, A_{t-1} = a \} $$

Several commonly use quantities:

state-transition probability
$$ p(s' \mid  s, a) = \sum_r p(s', r \mid s, a)$$

expected reward for state-action pair
$$
\begin{aligned}
 r(s, a)  &= E[R_t \mid S_{t-1} = s, A_{t - 1} = a]  \\
                &=\sum_{r \in \rewardset} r  \sum_{s' \in \stateset} p(s', r \mid s, a) \\
 \end{aligned}
$$

expected reward for state-action-next-state triples 

$$
\begin{aligned}
r(s, a, s') &= E[R_t \mid S_{t-1} = s, A_{t-1} = a, S_t = s']  \\
                    &= \sum_{r \in \rewardset} r  p(r \mid s, a, s') \\
                    &= \sum_{r \in \rewardset} r  \frac{p(s', r \mid s, a) } {p(s' \mid s, a, s')}
\end{aligned}
$$

For episodic tasks, we have nonterminal states $\stateset$ and terminal states $\termstateset$.  Time of termination $T$ is  a random variable between episodes.  

Returns at time step $t$ is 
$$
\begin{aligned}
G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \\
        &= R_{t+1} + \gamma G_{t+1} \\
\end{aligned}
$$

All reinforcement learning algorithm involve estimating 3 quantities: value function, state-action function.

A policy is a probability distribution on $\actionset$ given $s$ denoted as  $\pi (a \mid s)$.

A value function of a state $s$ under a policy $\pi$ is the expected return when starting in  $s$ and following $\pi$ thereafter.

$$ 
\begin{aligned}
v_{\pi}(s) &= E_{\pi} [G_t \mid S_t = s]  \\
                 &=  E_{\pi} \left[ \sum^{\infty}_{i=0}  \gamma^i R_{t + i + 1} \Mid S_t = s \right],  \hspace{2ex} \mbox{ (for all } s \in \stateset)  \\
                 &=  E_{\pi} \left[ R_{t+1} + \gamma \sum^{\infty}_{i=0} \gamma^i R_{(t+1) + i + 1} \Mid S_t = s \right] \\
                 &=  E_{\pi} [ R_{t+1} \mid S_t = s]  + \gamma  E_{\pi} [ G_{t+1} \mid S_t = s ] \\
                 &=  \sum_{a, r, s'} r p(s',  r \mid s , a) \pi(a \mid s) , \hspace{2ex} \mbox{(Law of Total Expectation)} \\ 
                 &\hspace{3ex} +  \gamma  \sum_{a, s'} E_{\pi}[G_{t + 1} \mid S_{t+1} = s'] p(s' \mid s, a)  \pi (a \mid s)  \\
                 &=  \sum_{a, r, s'} r p( r \mid s , a) \pi(a \mid s)  +  \gamma  \sum_{a, r, s'} v_{\pi}(s')  p(s', r \mid s, a)  \pi (a \mid s)  \\
                 &= \sum_{a} \pi(a \mid s)  \sum_{r, s'}   p(s', r \mid s, a)   \left[ r  + \gamma \sum_{s'}  v_{\pi}(s')  \right]
\end{aligned}
$$

The state-action function is the expected return  of taking an action $a$ at state $s$ before following the policy thereafter.
$$
\begin{aligned}
 q(s, a) &= E_{\pi} [G_t \mid S_t = s, A_t = a]  \\
             &= E_{\pi} \left[ \sum^{\infty}_{i=0}  \gamma^i R_{t + i + 1}  \Mid S_t = s , A_t = a\right]  \\
             &= E_{\pi} [R_{t+1} \mid S_t = s, A_t =  a] + \gamma E_{\pi} [ G_{t+1} \mid S_t = s, A_t =  a] \\
             &=  \sum_{s', r} r p(s', r  \mid s, a) + \gamma \sum_{s', a'} E_{\pi} [G_{t+1} \mid s', a'] p(s', a' \mid s, a) \\
             &=  \sum_{s', r} r p(s', r  \mid s, a) + \gamma \sum_{s', a'} q(s', a') p( a' \mid s',  s, a) p(s' \mid  s, a) \\
             &=  \sum_{s', r} r p(s', r  \mid s, a) + \gamma \sum_{s', a', r} q(s', a') \pi(a' \mid s')  p(s', r \mid  s, a) \\
             &=  \sum_{s', r}  p(s', r  \mid s, a) \left [ r + \gamma \sum_{a'} q(s', a') \pi(a' \mid s')  \right] 
 \end{aligned}
$$
\end{document}